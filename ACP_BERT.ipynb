{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x0oUsEG6gJYU",
        "QhCm1Gr0sdZo",
        "w6y_kiNzgAmQ",
        "C1TjekZrYfOv",
        "KPybdkjRflyX",
        "1NV7zUX2gSaK",
        "bu_GmHkWexps",
        "uzH-pECok-ll",
        "oMVCR0cbmKaS",
        "jZxowlbUqPGu",
        "Q5un4hK1eyec",
        "dsncvIi4cksI",
        "HjwAQ-C7wbHY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0oUsEG6gJYU"
      },
      "source": [
        "# Install needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddYwRMKmSsM9",
        "outputId": "711778b4-bb27-433d-b5ef-1e9ef1ddf854"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCm1Gr0sdZo"
      },
      "source": [
        "# Import needed libariries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nuk1mjcsWxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0a0dde-35f3-4ce7-8edc-ceeb8bbfabc4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "% matplotlib inline\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.backends.cudnn.deterministic = True \n",
        "torch.backends.cudnn.benchmark = False \n",
        "torch.backends.cudnn.enabled = False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(seed_val)\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6y_kiNzgAmQ"
      },
      "source": [
        "# Use GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gxfTmaR9Lk"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axb7b5L_SdsP"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1TjekZrYfOv"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRmlbW0kYdNk",
        "outputId": "3c50aa33-8f4d-41d3-b742-1d6b30b4a215"
      },
      "source": [
        "#drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-knx_qFLaQg2",
        "outputId": "60087df0-1b43-4e90-b41b-f535d6ce4745"
      },
      "source": [
        "domain = \"Social_Networking\"\n",
        "#domain = \"Games\"\n",
        "#domain = \"Productivity\"\n",
        "NUM_OF_CLASSES = 2\n",
        "\n",
        "domain = \"Social_Networking\"\n",
        "#domain = \"Games\"\n",
        "#domain = \"Productivity\"\n",
        "NUM_OF_CLASSES = 9\n",
        "\n",
        "X_train_url = \"\"\n",
        "y_train_url = \"\"\n",
        "X_test_url = \"\"\n",
        "y_test_url = \"\"\n",
        "\n",
        "if domain == \"Social_Networking\":\n",
        "  X_train_url = \"https://drive.google.com/uc?id=1-oze2ZVpgZ2hodIlZn7ro9Yh9xewBCH9&export=download\"\n",
        "  y_train_url = \"https://drive.google.com/uc?id=1-kszADYLrGNGrDWx8E7EUpHiyzix4-Iw&export=download\"\n",
        "  X_test_url = \"https://drive.google.com/uc?id=1-kjhYg0Nvv09JzETw876sViI8XTDpsF7&export=download\"\n",
        "  y_test_url = \"https://drive.google.com/uc?id=1-qbR1VBDBrjpLJCRHNJY7GOOHHMkKNif&export=download\"\n",
        "elif domain == \"Games\":\n",
        "  X_train_url = \"https://drive.google.com/uc?id=1-cP2d8l7OKxkPDvRT455qw4y7DHPdChR&export=download\"\n",
        "  y_train_url = \"https://drive.google.com/uc?id=1-Q-XNyBzcBEaa4u7G0XWYtFIVGEjeUK-&export=download\"\n",
        "  X_test_url = \"https://drive.google.com/uc?id=1-KstbnjDRQykVxBLp9JAoE-ZcXoBK06u&export=download\"\n",
        "  y_test_url = \"https://drive.google.com/uc?id=1-f7ONCkmpds20ZCrJepkInHr8ip8WNhF&export=download\"\n",
        "elif domain == \"Productivity\":\n",
        "  X_train_url = \"https://drive.google.com/uc?id=13sdG58GCMU3bvtOgpT6NVuFXyBlhO8cg&export=download\"\n",
        "  y_train_url = \"https://drive.google.com/uc?id=15F8IRKD2isn1fyFH4SF49k-3aZPU77su&export=download\"\n",
        "  X_test_url = \"https://drive.google.com/uc?id=1_ttANznHm8p39uUTtCKWbFe8rVqCk1Vd&export=download\"\n",
        "  y_test_url = \"https://drive.google.com/uc?id=1-18dZRGT2SC9aHCLJ6hsyMuzoSJICMPK&export=download\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Read data from csv\\ndf = pd.read_csv(\"/content/drive/MyDrive/AWARE/\"+domain+\\'.csv\\')\\ndf = df[df.is_opinion != False]\\ndf = df[[\"sentence\",\"sentiment\"]]\\ndf[\\'sentiment_id\\'] = df[\\'sentiment\\'].factorize()[0]\\n\\ndf.head()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(X_train_url,'rb') as pickle_file:\n",
        "    X_train = pickle.load(pickle_file)\n",
        "\n",
        "with open(y_train_url,'rb') as pickle_file:\n",
        "    y_train = pickle.load(pickle_file)\n",
        "\n",
        "with open(X_test_url,'rb') as pickle_file:\n",
        "    X_test = pickle.load(pickle_file)\n",
        "\n",
        "with open(y_test_url,'rb') as pickle_file:\n",
        "    y_test = pickle.load(pickle_file)"
      ],
      "metadata": {
        "id": "M2LoUc5f3zMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train['sentence'].values\n",
        "X_test = X_test['sentence'].values\n",
        "\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "sentences = np.concatenate([X_train,X_test])"
      ],
      "metadata": {
        "id": "nqsFgGD3IG2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "VLZm4gq9fUc0",
        "outputId": "6bfeb28f-5c98-499d-e051-8132b2492844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9FCwTWFa8tm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8970e992-0bc6-4e95-eed0-eac0de345f8c"
      },
      "source": [
        "'''\n",
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.sentiment_id.values\n",
        "\n",
        " X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.3, random_state=seed_val, stratify = labels)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Get the lists of sentences and their labels.\\nsentences = df.sentence.values\\nlabels = df.sentiment_id.values\\n\\n X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.3, random_state=seed_val, stratify = labels)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPybdkjRflyX"
      },
      "source": [
        "# Tokenization & Input Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR4f2LKubJz7",
        "outputId": "69a855fb-d0b2-4ef6-82ea-81949d3e9b8e"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4GdwZUpbTYj",
        "outputId": "695b7b45-6386-4e6b-c7d6-6a090bd19678"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BS74No8c2kV",
        "outputId": "05fd2173-e6bb-4236-b68d-22bc44ea0946"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in X_train: #sentences:\n",
        "  # `encode_plus` will:\n",
        "  #   (1) Tokenize the sentence.\n",
        "  #   (2) Prepend the `[CLS]` token to the start.\n",
        "  #   (3) Append the `[SEP]` token to the end.\n",
        "  #   (4) Map tokens to their IDs.\n",
        "  #   (5) Pad or truncate the sentence to `max_length`\n",
        "  #   (6) Create attention masks for [PAD] tokens.\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                      sent,                      # Sentence to encode.\n",
        "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                      max_length = max_len,           # Pad & truncate all sentences.\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,   # Construct attn. masks.\n",
        "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "  \n",
        "  # Add the encoded sentence to the list.    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "  \n",
        "  # And its attention mask (simply differentiates padding from non-padding).\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(y_train)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', X_train[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Facebook is been taking a lot of heat for privacy issues.\n",
            "Token IDs: tensor([ 101, 9130, 2003, 2042, 2635, 1037, 2843, 1997, 3684, 2005, 9394, 3314,\n",
            "        1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NV7zUX2gSaK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu_GmHkWexps"
      },
      "source": [
        "## Training & Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWoA7fUzeseK"
      },
      "source": [
        "def split_dataset(batch_size):\n",
        "  # Combine the training inputs into a TensorDataset.\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Create a 90-10 train-validation split.\n",
        "\n",
        "  # Calculate the number of samples to include in each set.\n",
        "  train_size = int(0.9 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  # Divide the dataset by randomly selecting samples.\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('{:>5,} training samples'.format(train_size))\n",
        "  print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "  # Create the DataLoaders for our training and validation sets.\n",
        "  # We'll take training samples in random order.\n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size, # Trains with this batch size.\n",
        "              num_workers = 0,\n",
        "              worker_init_fn = seed_worker,\n",
        "              generator = g\n",
        "          )\n",
        "  \n",
        "  \n",
        "\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  validation_dataloader = DataLoader(\n",
        "              val_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size, # Evaluate with this batch size.\n",
        "              num_workers = 0,\n",
        "              worker_init_fn = seed_worker,\n",
        "              generator = g\n",
        "          )\n",
        "\n",
        "  return train_dataloader, validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzH-pECok-ll"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBPlLa2mh-uZ"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlNy-8xQk8qt"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNw0FaNs8Vpl"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_xjuLu7lEHz"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "def train(learning_rate, batch_size, num_epochs, train_dataloader, validation_dataloader):\n",
        "  \n",
        "  # ========================================\n",
        "  #   Load BertForSequenceClassification\n",
        "  # ========================================\n",
        "\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "      num_labels = NUM_OF_CLASSES, # The number of output labels--2 for binary classification.\n",
        "                      # You can increase this for multi-class tasks.   \n",
        "      output_attentions = False, # Whether the model returns attentions weights.\n",
        "      output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "      return_dict=False\n",
        "  )\n",
        "\n",
        "  # Set number of epochs\n",
        "  epochs = num_epochs\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "\n",
        "  # ========================================\n",
        "  #               Create Adam Optemizer\n",
        "  # ========================================\n",
        "\n",
        "  # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "  # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                    lr = learning_rate, # args.learning_rate - default is 5e-5, \n",
        "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "    \n",
        "  # ========================================\n",
        "  #               Create Scheduler\n",
        "  # ========================================\n",
        "\n",
        "\n",
        "  # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "  # (Note that this is not the same as the number of training samples).\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss, \n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, epochs):\n",
        "      \n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      \n",
        "      # Perform one full pass over the training set.\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      total_train_loss = 0\n",
        "\n",
        "      # Put the model into training mode. Don't be mislead--the call to \n",
        "      # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "      # `dropout` and `batchnorm` layers behave differently during training\n",
        "      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          #if step == 0:\n",
        "          #  print(type(batch))\n",
        "          #  print(batch[0])\n",
        "\n",
        "          # Progress update every 40 batches.\n",
        "          if step % 40 == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "          # `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a\n",
        "          # backward pass. PyTorch doesn't do this automatically because \n",
        "          # accumulating the gradients is \"convenient while training RNNs\". \n",
        "          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "          model.zero_grad()        \n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          # The documentation for this `model` function is here: \n",
        "          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "          # It returns different numbers of parameters depending on what arguments\n",
        "          # arge given and what flags are set. For our useage here, it returns\n",
        "          # the loss (because we provided labels) and the \"logits\"--the model\n",
        "          # outputs prior to activation.\n",
        "          loss, logits = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask, \n",
        "                              labels=b_labels)\n",
        "     \n",
        "          # Accumulate the training loss over all of the batches so that we can\n",
        "          # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "          # single value; the `.item()` function just returns the Python value \n",
        "          # from the tensor.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0.\n",
        "          # This is to help prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters and take a step using the computed gradient.\n",
        "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "          # modified based on their gradients, the learning rate, etc.\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          scheduler.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our validation set.\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in validation_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          #\n",
        "          # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "          # the `to` method.\n",
        "          #\n",
        "          # `batch` contains three pytorch tensors:\n",
        "          #   [0]: input ids \n",
        "          #   [1]: attention masks\n",
        "          #   [2]: labels \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # token_type_ids is the same as the \"segment ids\", which \n",
        "              # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "              # The documentation for this `model` function is here: \n",
        "              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "              # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "              # values prior to applying an activation function like the softmax.\n",
        "              (loss, logits) = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "              \n",
        "          # Accumulate the validation loss.\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          # Move logits and labels to CPU\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences, and\n",
        "          # accumulate it over all batches.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "          \n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "      # Save performance of this epoch\n",
        "      if epoch_i > 0:\n",
        "        export_training_summary(learning_rate, epoch_i + 1, batch_size, training_stats)\n",
        "        #save_model(model, learning_rate,  epoch_i + 1, batch_size)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMVCR0cbmKaS"
      },
      "source": [
        "## Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Bft5rDmN1i"
      },
      "source": [
        "def export_training_summary(learning_rate, epoch, batch_size, training_stats):\n",
        "  # Display floats with two decimal places.\n",
        "  pd.set_option('precision', 2)\n",
        "\n",
        "  # Create a DataFrame from our training statistics.\n",
        "  df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "  # Use the 'epoch' as the row index.\n",
        "  df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "  # A hack to force the column headers to wrap.\n",
        "  #df_stats = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "  df_stats.to_csv(\"/content/drive/MyDrive/RE/Results/DL/ACP/\"+domain+'_'+str(learning_rate)+'_'+str(epoch)+'_'+str(batch_size)+'.csv')\n",
        "\n",
        "  # Display the table.\n",
        "  print(df_stats.head())\n",
        "\n",
        "  #plot_loss(df_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ-Sxgodmp-X"
      },
      "source": [
        "def plot_loss(df_stats):\n",
        "  # Use plot styling from seaborn.\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "  # Plot the learning curve.\n",
        "  plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "  plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "  # Label the plot.\n",
        "  plt.title(\"Training & Validation Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig('/content/drive/MyDrive/RE/Results/DL/ACP/valid_train_loss.png', dpi=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZxowlbUqPGu"
      },
      "source": [
        "# Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qmnozJUqSxv"
      },
      "source": [
        "'''\n",
        "def save_model(model, learning_rate, epoch, batch_size):\n",
        "  # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "  output_dir = '/content/drive/MyDrive/RE/Results/DL/ACP/'+domain+'/'+str(learning_rate)+'_'+str(epoch)+'_'+str(batch_size)\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "      os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "  # They can then be reloaded using `from_pretrained()`\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  # Good practice: save your training arguments together with the trained model\n",
        "  # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5un4hK1eyec"
      },
      "source": [
        "# Hyper-parameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7toWLUqe4_r",
        "outputId": "eac7d551-b727-4414-c2fa-2630f2b3654d"
      },
      "source": [
        "LEARNING_RATES = [2e-5, 3e-5, 5e-5]\n",
        "#EPOCHS = [2, 3, 4] # The BERT authors recommend between 2 and 4. \n",
        "BATCH_SIZES = [16, 32] #For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
        "\n",
        "for BATCH_SIZE in BATCH_SIZES:\n",
        "  train_dataloader, validation_dataloader = split_dataset(BATCH_SIZE)\n",
        "  #for EPOCH in EPOCHS:\n",
        "  for LEARNING_RATE in LEARNING_RATES:\n",
        "    model = train(LEARNING_RATE, BATCH_SIZE, 4, train_dataloader, validation_dataloader)\n",
        "      \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,963 training samples\n",
            "  219 validation samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:26.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.43\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.39         0.43           0.80       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.39         0.43           0.80       0:00:26         0:00:01\n",
            "3               0.27         0.47           0.80       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:27.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:27\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.39         0.43           0.80       0:00:26         0:00:01\n",
            "3               0.27         0.47           0.80       0:00:26         0:00:01\n",
            "4               0.20         0.55           0.79       0:00:27         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:49 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.33         0.48           0.82       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.33         0.48           0.82       0:00:26         0:00:01\n",
            "3               0.22         0.56           0.82       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.48           0.78       0:00:26         0:00:01\n",
            "2               0.33         0.48           0.82       0:00:26         0:00:01\n",
            "3               0.22         0.56           0.82       0:00:26         0:00:01\n",
            "4               0.14         0.70           0.80       0:00:26         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:47 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.51\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.44           0.80       0:00:26         0:00:01\n",
            "2               0.33         0.51           0.79       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.44           0.80       0:00:26         0:00:01\n",
            "2               0.33         0.51           0.79       0:00:26         0:00:01\n",
            "3               0.20         0.65           0.80       0:00:26         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    123.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    123.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    123.    Elapsed: 0:00:25.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.77\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.53         0.44           0.80       0:00:26         0:00:01\n",
            "2               0.33         0.51           0.79       0:00:26         0:00:01\n",
            "3               0.20         0.65           0.80       0:00:26         0:00:01\n",
            "4               0.13         0.77           0.80       0:00:26         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:47 (h:mm:ss)\n",
            "1,963 training samples\n",
            "  219 validation samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.39\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.58         0.39           0.85       0:00:20         0:00:01\n",
            "2               0.43         0.41           0.81       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.43\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.58         0.39           0.85       0:00:20         0:00:01\n",
            "2               0.43         0.41           0.81       0:00:20         0:00:01\n",
            "3               0.34         0.43           0.81       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.58         0.39           0.85       0:00:20         0:00:01\n",
            "2               0.43         0.41           0.81       0:00:20         0:00:01\n",
            "3               0.34         0.43           0.81       0:00:20         0:00:01\n",
            "4               0.28         0.47           0.80       0:00:20         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:23 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.42\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.56         0.45           0.82       0:00:20         0:00:01\n",
            "2               0.40         0.42           0.84       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.56         0.45           0.82       0:00:20         0:00:01\n",
            "2               0.40         0.42           0.84       0:00:20         0:00:01\n",
            "3               0.29         0.50           0.81       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.53\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.56         0.45           0.82       0:00:20         0:00:01\n",
            "2               0.40         0.42           0.84       0:00:20         0:00:01\n",
            "3               0.29         0.50           0.81       0:00:20         0:00:01\n",
            "4               0.22         0.53           0.80       0:00:20         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:22 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.43\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.43           0.82       0:00:20         0:00:01\n",
            "2               0.35         0.45           0.82       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.43           0.82       0:00:20         0:00:01\n",
            "2               0.35         0.45           0.82       0:00:20         0:00:01\n",
            "3               0.23         0.54           0.80       0:00:20         0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.54         0.43           0.82       0:00:20         0:00:01\n",
            "2               0.35         0.45           0.82       0:00:20         0:00:01\n",
            "3               0.23         0.54           0.80       0:00:20         0:00:01\n",
            "4               0.16         0.64           0.78       0:00:20         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:23 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-train with the best hyperparameters"
      ],
      "metadata": {
        "id": "dsncvIi4cksI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(2e-05, 32, 2, train_dataloader, validation_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUmFQZtj9DaB",
        "outputId": "4422baaf-dbc6-417f-bf42-c2ac02485c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of     62.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.39\n",
            "  Validation took: 0:00:01\n",
            "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
            "epoch                                                                         \n",
            "1               0.57         0.41           0.84       0:00:20         0:00:01\n",
            "2               0.40         0.39           0.83       0:00:20         0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:42 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "HjwAQ-C7wbHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence and label lists\n",
        "#sentences = df.sentence.values\n",
        "#labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in X_test:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        #pad_to_max_length = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(y_test)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "I61dMcmvwc93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "#model.cuda()\n",
        "model.eval()\n",
        "\n",
        "#loaded_model.cuda()\n",
        "#loaded_model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  #batch = tuple(t for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO7NLzhh5vvj",
        "outputId": "ac5b0230-e516-4507-c49b-4a4c9eece579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 936 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "\n",
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the acc, recall, precision, and f1-score\n",
        "acc = accuracy_score(flat_true_labels, flat_predictions)\n",
        "recall = recall_score(flat_true_labels, flat_predictions)\n",
        "precision = precision_score(flat_true_labels, flat_predictions)\n",
        "f1 = f1_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total Accuracy: %.9f' % acc)\n",
        "print('Total recall: %.9f' % recall)\n",
        "print('Total precision: %.9f' % precision)\n",
        "print('Total f1: %.9f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO7djjGZ8LeW",
        "outputId": "89056b43-f0de-4dce-9755-e3903e012e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Accuracy: 0.818376068\n",
            "Total recall: 0.751196172\n",
            "Total precision: 0.826315789\n",
            "Total f1: 0.786967419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating accuracy for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  accuracy = accuracy_score(true_labels[i], pred_labels_i)                \n",
        "  accuracy_set.append(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Cjd01V7GJS",
        "outputId": "f0896e74-83f7-470e-b6ed-b4cc3cf6ad07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating accuracy for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(accuracy_set))), y=accuracy_set, ci=None)\n",
        "\n",
        "plt.title('Accuracy Score per Batch')\n",
        "plt.ylabel('Accuracy (0 to 1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nurT-GD-78S4",
        "outputId": "902e0c81-453d-42d4-8393-f5e93eccf939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debwcVZn/8c+XhEhYNCBhSwJhBhAQZTGAzCAqoIY1rA6MyCKCiBEUB4XRH+PAuAAjLsgiOyqbAmKASECEwYUlAQIkIBAwSCJLgCAgCASe3x/nXFLpVFV34q2+N8n3/Xr169by9KnTp/rW03VqU0RgZmZLtqX6ugJmZtb3nAzMzMzJwMzMnAzMzAwnAzMzw8nAzMxwMjCzhkkaKSkkDezrulg1JwNrS9LNkmZLeltf16Upkv5T0p8kvSRphqTL+rpOTSpsoF/Kr6cknS5p6Q7ff6Ck3zVdT+seJwOrJWkk8AEggF27vOyu/JKUdADwSWD7iFgeGAXc2MvL6LNfxW2WPSR/5vcAWwGf606trL9xMrB29gduAy4ADijOkDRC0pWSZkl6VtIPC/MOkfSApBcl3S9pszw9JK1TiLtA0v/k4Q/lX+VfkfQkcL6kFSVdk5cxOw8PL7x/JUnnS/pLnn9Vnj5F0i6FuKUlPSNp05LPuDkwISIeAYiIJyPirHbLKHzOaZKekzRO0hqFeSHpc5IeBh7O03aWNFnS85L+IOm9VQ2f33+EpEdz3U+WtFRh/qdyG8+WNEHSWnXLrhMRTwM3ABsWyjhG0iOFdbh7nr4BcCawVd6reD5PHyzpO5Iek/RXSb+TNLiwmE9I+nP+LF9tVyfrsojwy6/KFzANOBx4H/A6sGqePgC4B/gusBywDLB1nrc3MJO0kRWwDrBWnhfAOoXyLwD+Jw9/CJgDnAi8DRgMvBPYE1gWWAH4OXBV4f3XApcBKwJLAx/M078MXFaIGwPcV/EZ9wOeA44m7RUMaJlftYxtgWeAzXJ9TwVuKbwvSBvYlfJn2RR4Gtgyt98BwHTgbRX1CuCm/P41gYeATxc+zzRgA2Ag8DXgD1XLLil7ZI4ZmMfXyOvzU4WYvfP0pYB/A/4GrJ7nHQj8rqXM04CbgWH58/1LbpeeZZ2d22Fj4FVgg77+fvtVWH99XQG/+u8L2JqUAFbO438EvpiHtwJm9WxMWt43ATiyosx2yeA1YJmaOm0CzM7DqwNvAiuWxK0BvAi8PY9fDny5ptxPAL/OG7xnga90sIxzgZMK48vn9hpZ+KzbFuafAZzQUsaD5ORS0VajC+OHAzfm4V8BBxfmLQW8zLxJd9uaz9uzgX4+vwL4Q097VbxnMjAmD8+TDPLyXwE2rlnW8MK0O4B9+vo77tfcl7uJrM4BwPUR8Uwev5i5XUUjgMciYk7J+0YAjyzkMmdFxN97RiQtK+lHuevhBeAWYIikAXk5z0XE7NZCIuIvwO+BPSUNAXYALqpaaERcFBHbA0OAw4ATJH2sbhmkhPNYoYyXSIlkWCHm8cLwWsCXchfR87l7ZUQup0rx/Y8VYtcCvl8o5znSXljVsqusHBFDSHtevyclcgAk7V/o0noe2AhYuaoc0t5h3Xp/sjD8Mil5Wj/hZGClcl/vx4EPSnoy9+F/EdhY0sakDc2aFQcnHwf+uaLol0kbnh6rtcxvvY3ul4B3AVtGxNuBbXqqmJezUt7Yl7mQ1AW0N3BrRMysiJu78IjXI+LnwL2kjV/dMv5C2iinCknLkbq1isspfp7HgW9ExJDCa9mIuKSmSiMKw2vmZfaU9ZmWsgZHxB8qll0rIl4h7aW9X9LK+fjD2cBY4J05YUwhtXtZ2c8Af6d6vVs/52RgVXYD3iAdUNwkvzYAfks6qHwH8ATwbUnLSVpG0r/m954D/Iek9ylZp3BwczLw75IGSBoNfLBNPVYgdT88L2kl4L96ZkTEE6TuktPzgealJW1TeO9VpP78I4EfVy1A6TTJnSStIGkpSTsA7wZub7OMS4CDJG2idNrtN/N7plcs6mzgMElb5nZZrme5NZ//6LzcEflz9JzyeiZwrKR358/wDkl715RTK9f/k6Rf78+SjgMFqSsQSQeRkmOPp4DhkgYBRMSbwHnAKZLWyOt3Ky3GpyMvdvq6n8qv/vkCrgO+UzL946QNxkDSL9WrSBuPZ4AfFOIOI/WHv0T6Rblpnj4KmErqz/8JaYNaPGYwo2V5a5AOSr5EOoD6GeY98LkSaQ/gKWA2cGXL+88hHQdYvuaz7kHqIpkNvADcBxxYmF+5jPw5HyF101zDvP3i8xwfydNGAxNJ/fRPkA6Ir1BRrwCOAB7NbfwdCge3SRvv+3KdHwfOq1t2S9kjc8xL+fU88H/A5oWYb+TP9QxwSp7fcwB7EOnA+nPAM3naYOB7pD2jv5K69AbTcrA6x97cU5Zf/eOlvGLMFkuSjgPWi4j9+rouC0pSAOtGxLS+rost/nx5uC22crfSwaRf0GZWw8cMbLEk6RBS18mvIuKWvq6PWX/nbiIzM/OegZmZLYLHDFZeeeUYOXJkX1fDzGyRcueddz4TEUOr5i9yyWDkyJFMmjSpr6thZrZIkfRY3Xx3E5mZmZOBmZk5GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZmxCF6BbGbW3z15yn2V81Y76j1drEnnvGdgZmZOBmZm5mRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmRsPJQNJoSQ9KmibpmJL535U0Ob8ekvR8k/UxM7Nyjd2OQtIA4DTgI8AMYKKkcRFxf09MRHyxEP95YNOm6mNmZtWa3DPYApgWEY9GxGvApcCYmvh9gUsarI+ZmVVoMhkMAx4vjM/I0+YjaS1gbeA3DdbHzMwq9JcDyPsAl0fEG2UzJR0qaZKkSbNmzepy1czMFn9NJoOZwIjC+PA8rcw+1HQRRcRZETEqIkYNHTq0F6toZmbQbDKYCKwraW1Jg0gb/HGtQZLWB1YEbm2wLmZmVqOxs4kiYo6kscAEYABwXkRMlXQ8MCkiehLDPsClERFN1aXOE6f/v9r5qx9+QpdqYmbWdxp90llEjAfGt0w7rmX8603WwczM2usvB5DNzKwPORmYmZmTgZmZORmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmNPw8AzODnS+/rHb+NXv9W5dqsmS6+5yna+dv+ulVulST/s17BmZm5mRgZmZOBmZmRsPJQNJoSQ9KmibpmIqYj0u6X9JUSRc3WR8zMyvX2AFkSQOA04CPADOAiZLGRcT9hZh1gWOBf42I2ZJ8JMfMrA80uWewBTAtIh6NiNeAS4ExLTGHAKdFxGyAiKg/7G9mZo1oMhkMAx4vjM/I04rWA9aT9HtJt0kaXVaQpEMlTZI0adasWQ1V18xsydXXB5AHAusCHwL2Bc6WNKQ1KCLOiohRETFq6NChXa6imdnir8mLzmYCIwrjw/O0ohnA7RHxOvAnSQ+RksPEBuu1UB77wW6V89Y64qoFKuu3Z+9cO/8Dh1yzQOWZLYy9rphcO//yPTfpUk2sP2hyz2AisK6ktSUNAvYBxrXEXEXaK0DSyqRuo0cbrJOZmZVoLBlExBxgLDABeAD4WURMlXS8pF1z2ATgWUn3AzcBR0fEs03VyczMyjV6b6KIGA+Mb5l2XGE4gKPyy8zM+khfH0A2M7N+wMnAzMycDMzMzMnAzMxYzB9uM+vM0yrnDT3sc12siVnvGHP5hNr5v9zrY12qiXXL06feWDlvlc9v12vL8Z6BmZk5GZiZmZOBmZnhZGBmZjgZmJkZTgZmZkYHp5ZKGgV8AFgDeAWYAtzQ83QyMzNb9FXuGUg6SNJdpGcUDwYeBJ4GtgZ+LelCSWt2p5pmZtakuj2DZUkPqn+lbKakTUgPovlzExWzJcNuvyx90ikAV425rpFl7nTl9yrnXbvHFxaorJ2v+HHlvGv23H+ByupNu1/xu9r5v9hz6y7VxBYVlckgIqov303z6x+TZGZmi4yFOoAs6bj2UWZmtqhY2LOJPt2rtTAzsz5V2U0k6YWqWaQDymZmtpioO4D8PLB5RDzVOkPS481VyczMuq2um+jHwFoV8y7upHBJoyU9KGmapGNK5h8oaZakyfnl7iczsz5QdzbR12rmfaVdwZIGAKcBHwFmABMljYuI+1tCL4uIsR3W18zMGtDk7Si2AKZFxKMR8RpwKTCmweWZmdlCavJJZ8OA4rGFGcCWJXF7StoGeAj4YkTMdzxC0qHAoQBrrtl/L3qefMautfM3+ey4jsu67twda+ePPng8AJefX33RFsBeBzVz4VY7X768vl4n7dV5vXa4qv5CsF/tVn0RmVlvevLkx2rnr3Z0Vc96/9fXN6q7GhgZEe8FbgAuLAuKiLMiYlREjBo6dGhXK2hmtiToaM9A0sakm9UB/DYi7ungbTOBEYXx4XnaWyLi2cLoOcBJndTHzMx6V9s9A0lHAhcBq+TXTyV9voOyJwLrSlpb0iBgH2CefhJJqxdGdwUe6LTiZmbWezrZMzgY2DIi/gYg6UTgVuDUujdFxBxJY4EJwADgvIiYKul4YFJEjAOOkLQrMAd4DjhwoT+JmZkttE6SgYA3CuNv5GltRcR4YHzLtOMKw8eSbpFtZmZ9qJNkcD5wu6Rf5PHdgPOaq5KZmXVb22QQEadIupn0UBuAgyLi7kZrZWZmXdXJYy9/EhGfBO4qmWaLkdN/+rHa+YfvN6FLNVk4O/7if2rnj9+98qL6+ex0xVm186/d89COy+rULpf/snLe1Xv13fWan7iy/tz6i/bom3Prb/nprNr52+zXv09Df+r7t9XOX/XI93epJkkn1xm8uziSbzPxvmaqY2ZmfaHuGcjHSnoReK+kF/LrRdJzkKt/wpiZ2SKnMhlExLciYgXg5Ih4e36tEBHvzGcBmZnZYqJtN5E3/GZmi7++vjeRmZn1A04GZmZWf2qpJJGeSzAsT5oJ3BER0XTFzMyseyqTgaSPAqcDDzP3bqPDgXUkHR4R13ehfmZm1gV1ewbfB7aPiOnFiZLWJt1vaIMG61Vr1hk/rZ0/9LP7dakmi44LL/ho5bwDDnRet/7l6p89Uzlvl4+v3MWaLDnqjhkMJD2drNVMYOlmqmNmZn2hbs/gPNJD7C9l7uMrR5CeS3Bu0xUzM7PuqUwGEfEtSVeRHmK/VZ48E/hERNzfjcqZmVl31J5NFBEP4KePmZkt9uruTXS1pF0kzXd8QNI/STpe0qearZ6ZmXVD3Z7BIcBRwPckPQfMApYBRgKPAD+MCN+wzsxsMVB3zOBJ4MvAlyWNBFYHXgEeioiXOylc0mjSKaoDgHMi4tsVcXsClwObR8SkBfkAZmb2j+vksZfkaw2mL0jB+bkHpwEfIZ2iOlHSuNaDz5JWAI4Ebl+Q8s3MrPd0lAwW0hbAtIh4FCCfojoGaD0T6QTgRODoButivejES6ufiPaVffr309DMyjzygydr5//zEat1qSZ9p8kb1Q1j7vUJkPYOhhUDJG0GjIiIa+sKknSopEmSJs2aVf+oOzMzW3Btk0E+o6jXk0Yu8xTgS+1iI+KsiBgVEaOGDu3fzzU1M1sUdbKR/zfgYUknSVp/AcqeSbpiucdw5t7wDmAFYCPgZknTgfcD4ySNWoBlmJlZL+jkSWf7AZuSTie9QNKtudtmhTZvnQisK2ltSYNIt7EYVyj3rxGxckSMjIiRwG3Arj6byMys+zrq/omIF0infl5KOsV0d+AuSZ+vec8cYCwwgXQV888iYmq+WG3Xf7jmZmbWa9qeTZQ33AcB6wA/BraIiKclLUs6M+jUqvdGxHjS7a6L046riP1Q59U2M7Pe1MmppXsC342IW4oTI+JlSQc3Uy0zM+umTpLB14EnekYkDQZWjYjpEXFjUxUzs8XPhVfWnxp+wB4+W7CvdHLM4OfAm4XxN/I0MzNbTHSSDAZGxGs9I3l4UHNVMjOzbuskGcwqnv0jaQxQ/YBSMzNb5HRyzOAw4CJJPwREusXE/o3WyszMuqptMoiIR4D3S1o+j7/UeK3MzKyrOrprqaSdgHcDy0gCICKOb7BeZmbWRZ3cqO5M0v2JPk/qJtobWKvhepmZWRd1cgD5XyJif2B2RPw3sBWwXrPVMjOzbuokGfw9/31Z0hrA66T7E5mZ2WKik2MGV0saApwM3AUEcHajtTIzs66qTQb5ATQ3RsTzwBWSrgGWiYi/dqV2ZmbWFbXdRBHxJumh9j3jrzoRmJktfjo5ZnCjpD3Vc06pmZktdjpJBp8h3ZjuVUkvSHpR0gsN18vMzLqokyuQ2z3e0szM+tDTp42rnb/K59o/XLKTJ51tUza99WE3Zma26Ork1NKjC8PLAFsAdwLbNlIjMzPrurbHDCJil8LrI8BGwOxOCpc0WtKDkqZJOqZk/mGS7pM0WdLvJG244B/BzMz+UZ0cQG41A9igXZCkAaTTUncANgT2LdnYXxwR74mITYCTgFMWoj5mZvYP6uSYwamkq44hJY9NSFcit7MFMC0iHs3lXAqMAe7vCYiI4llJyxWWY2ZmXdTJMYNJheE5wCUR8fsO3jeM9CCcHjOALVuDJH0OOIr0KM3S4xCSDgUOBVhzzTU7WLSZmS2ITpLB5cDfI+INSN0/kpaNiJd7owIRcRpwmqR/B74GHFAScxZwFsCoUaO892Bm1ss6ugIZGFwYHwz8uoP3zQRGFMaH52lVLgV266BcMzPrZZ0kg2WKj7rMw8t28L6JwLqS1pY0CNgHmOfKCEnrFkZ3Ah7uoFwzM+tlnXQT/U3SZhFxF4Ck9wGvtHtTRMyRNBaYAAwAzouIqZKOByZFxDhgrKTtSc9ImE1JF5GZmTWvk2TwBeDnkv5CeuzlaqTHYLYVEeOB8S3TjisMH9l5Vc3MrCmd3JtooqT1gXflSQ9GxOvNVsvMzLqp7TGDfOrnchExJSKmAMtLOrz5qpmZWbd0cgD5kPykMwAiYjZwSHNVMjOzbuskGQwoPtgm32ZiUHNVMjOzbuvkAPJ1wGWSfpTHP5OnmZnZYqKTZPAV0q0gPpvHbwDObqxGZmbWdZ3cwvrNiDgzIvaKiL1IN5o7tfmqmZlZt3SyZ4CkTYF9gY8DfwKubLJSZmbWXZXJQNJ6pASwL/AMcBmgiPhwl+pmZouQb/3iicp5x+6+ehdrYgujbs/gj8BvgZ0jYhqApC92pVZmZtZVdccM9gCeAG6SdLak7Ui3ozAzs8VMZTKIiKsiYh9gfeAm0j2KVpF0hqSPdquCZmbWvE7OJvpbRFwcEbuQnklwN+l0UzMzW0x0cgXyWyJidkScFRHbNVUhMzPrvgVKBmZmtnhyMjAzMycDMzNzMjAzMxpOBpJGS3pQ0jRJx5TMP0rS/ZLulXSjpLWarI+ZmZVrLBnk5x6cBuwAbAjsK2nDlrC7gVER8V7gcuCkpupjZmbVmtwz2AKYFhGPRsRrwKXAmGJARNwUES/n0dtI1zGYmVmXNZkMhgGPF8Zn5GlVDgZ+1WB9zMysQke3sG6apP2AUcAHK+YfSnrADmuuuWYXa2ZmtmRocs9gJjCiMD48T5uHpO2BrwK7RsSrZQXlq55HRcSooUOHNlJZM7MlWZPJYCKwrqS1JQ0C9gHGFQPyQ3N+REoETzdYFzMzq9FYMoiIOcBYYALwAPCziJgq6XhJu+awk4HlgZ9LmixpXEVxZmbWoEaPGUTEeGB8y7TjCsPbN7l8MzPrjK9ANjMzJwMzM3MyMDMznAzMzAwnAzMzw8nAzMxwMjAzM5wMzMwMJwMzM8PJwMzMcDIwMzOcDMzMDCcDMzPDycDMzHAyMDMznAzMzAwnAzMzw8nAzMxwMjAzMxpOBpJGS3pQ0jRJx5TM30bSXZLmSNqrybqYmVm1xpKBpAHAacAOwIbAvpI2bAn7M3AgcHFT9TAzs/YGNlj2FsC0iHgUQNKlwBjg/p6AiJie573ZYD3MzKyNJruJhgGPF8Zn5GkLTNKhkiZJmjRr1qxeqZyZmc21SBxAjoizImJURIwaOnRoX1fHzGyx02QymAmMKIwPz9PMzKyfaTIZTATWlbS2pEHAPsC4BpdnZmYLqbFkEBFzgLHABOAB4GcRMVXS8ZJ2BZC0uaQZwN7AjyRNbao+ZmZWrcmziYiI8cD4lmnHFYYnkrqPzMysDy0SB5DNzKxZTgZmZuZkYGZmTgZmZoaTgZmZ4WRgZmY4GZiZGU4GZmaGk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZjgZmJkZTgZmZoaTgZmZ4WRgZmY4GZiZGQ0nA0mjJT0oaZqkY0rmv03SZXn+7ZJGNlkfMzMr11gykDQAOA3YAdgQ2FfShi1hBwOzI2Id4LvAiU3Vx8zMqjW5Z7AFMC0iHo2I14BLgTEtMWOAC/Pw5cB2ktRgnczMrIQiopmCpb2A0RHx6Tz+SWDLiBhbiJmSY2bk8UdyzDMtZR0KHJpH3wU82LK4lYFnqNdJTG/HLQll9cUyl4Sy+mKZ/bWsvlhmfy3rH1nmWhExtPIdEdHIC9gLOKcw/knghy0xU4DhhfFHgJUXYlmTeiOmt+OWhLIW9fr317IW9fq7LRaNtii+muwmmgmMKIwPz9NKYyQNBN4BPNtgnczMrESTyWAisK6ktSUNAvYBxrXEjAMOyMN7Ab+JnNbMzKx7BjZVcETMkTQWmAAMAM6LiKmSjiftwowDzgV+Imka8BwpYSyMs3opprfjloSy+mKZS0JZfbHM/lpWXyyzv5bV28t8S2MHkM3MbNHhK5DNzMzJwMzMaO7U0m68gNGkaw6mAcdUxJwHPA1MaVPWCOAm4H5gKnBkScwywB3APTnmv2vKGwDcDVxTEzMduA+YTM2pYMAQ0kV5fwQeALZqmf+uXEbP6wXgCxVlfTHXfQpwCbBMScyRef7UYjllbQmsBNwAPJz/rlgRt3cu701gVEXMyfkz3gv8In/usrgTcsxk4Hpgjbr1DHwJCOCikrK+TjqrraftdqwqC/h8rt/UvPzWsi4rlDM9/y2r/ybAbT3rnXQiRWvMxsCt+ftxNfB2Kr6jLevgFuC3JTGt7V9VVnEdXFdRVmv7jyorq6T9f1dSVmv7719VVkv7n1FR/+I6eBx4sSSmtf13riiruA6uzbHz/O8DawO3k7ZBlwErULKNAMbmmACGVcRcRNqeTSF9b5aviDs3T7uXtF14Z1lcod1+ALzUdnvaFxvx3niRNraPAP8EDMoNsWFJ3DbAZrRPBqsDm+XhFYCHWssDBCyfh5fOX4L3V5R3FHAx7ZNB2+sqSFdpfzoPDwKGtGmXJ0kXmLTOGwb8CRicx38GHNgSs1H+Mi5LOsHg18A6VW0JnEROxMAxpFuKlMVtQEpaN5M2HmUxHwUG5uETa8p6e2H4CODMqvVM2uhNAB4Ddikp6+vAf7T7zgAfzm3xtjy+a933CvgOcFxFWdcDO+ThHUk/GlpjJgIfzMOfIm2AS7+jLevgG8AFJTGt7V9VVnEd/LCirNb2/3FZWS3tPwPYtqSsedq/pl6t7b9R1TILZZ0JnFFSVmv7/6Fima3r4Nut//uk/6F9Csv7LCXbCGBTYCT5f74iZkfSNkakH2pVZRXb/xTS/13pdimv65/QQTJYlLuJOrndBRFxC+lMpVoR8URE3JWHXyT9Ah/WEhMR8VIeXTq/5jsCL2k4sBNwzgJ9ohKS3kHaoJyb6/BaRDxf85btgEci4rGK+QOBwfm6jmWBv7TM3wC4PSJejog5wP8Be+Rll7Vl8ZYiFwK7lcVFxAMR8WBhvCzm+rxMSL/chlfEvVAYXS5NqlzP3wW+TFpPt1bEzKOirM+SNgav5phxVWXlW6p8HLikoqwg/dKHdG3NAyUx65F+5UP61b9nzXe0uA5+CGzVGlPS/qVltayDG0nf8daY1vb/W83/Tk/7zyH9kq38/6qrF/O3/5S6/9e8DnYibSxbY1rbf3pFWa3rYNc8XPzf35b06xzmfv/n20ZExN0RMb3wOctixudtTJB+6Q+viHuh8BkH52nzxeX7w52c27+tRTkZDCPtBvaYQcmXa2Hku6duSsqwrfMGSJpM2q2/ISLmiwG+R1oBb7ZZVADXS7oz33KjzNrALOB8SXdLOkfScjVl7kP6VTH/wiJmAv8L/Bl4AvhrRFzfEjYF+ICkd0palvRrZQTVVo2IJ/Lwk8CqNbEL4lPAr6pmSvqGpMeBT5B+gZfFjAFmRsQ9bZY1VtK9ks6TtGJFzHqkdrld0v9J2rymvA8AT0XEwxXzvwCcnOv/v8CxJTFTmfvjZm9a1kHLd7R0HdR9j2vKKnprHbTGVLV/Ma6q/UuWV9r+LXGV7V9R/3nWQUtMZfu3xM23Dor/+6SeiecLyXMGMKyTbURdjKSlSXdsuK4qTtL5pHW9PnBqRdxYYFzhu1Gv3a5Df33Rwe0uCvNG0qabqBC7PHAnsEebuCGkfsaNWqbvDJyehz9EfTfRsPx3FVI31zYlMaNIv6q2zOPfB06oKG8Q6X4kq1bMXxH4DTCU9OvhKmC/kriDcxvcQuqb/V5VW5L+GYrvnV3X5uRuijYxXyUdM1BdXJ53LHP7Ut+KI+313A68I49PJ+2et9Z/VVLX2lKkLpbzKj7nFOBU0i78FqTutqr6nwF8qabNfkD6pQ9pD+LXJTHrk7oz7gT+C3i26jtatg5aY8rav+77XlwHVTEl7f9WXE37t9a9qv1b48rav7JuxXVQUtZ87V8RV7oOmPu/vzWpd6JnmSNa1uF82whauoYrYs6m8D9XEzcAOB04qCRuG9Ixmp4uv8X6mMFWwISWL+WxFbEj6SAZkDaQE4CjOqzDcczf3/wt0i+E6aTM/TLw0w7K+nprWXn6aqTd2J7xDwDXVpQxBri+Zhl7A+cWxvcnJ66a93wTOLyqLUkHvFbPw6sDD9a1OW2SAXAgqTtn2U7WH7AmcxPAyMLwe0i/kqbn1xzSHtHmNWWNLCsrj18HfLgw/gjlxygGAk8x7z23Wsv6K3MTnUgH/Os+43rAHVXf0bJ10BpT0f6l3/fiOqiKaW3/1ria9r+ppqyRZaNNGmYAAAT5SURBVGXVtP/qFfV/ax1UlFXW/u0+51vroPC/fzTpx1fPBneebVLZNoKS44TFGFLSuQpYqqQOZdubbWj5wZnj/ou0/elp/zcpJK6y16LcTdTJ7S46lvvfzgUeiIhTKmKGShqShwcDHyGd3fCWiDg2IoZHxMhcp99ExH4lZS0naYWeYdKBuymtcRHxJPC4pHflSduRznoosy8VXUTZn4H3S1o2f97tSP2jrXVbJf9dk/Qr7+KaMou3FDkA+GVNbC1Jo0nda7tGxMs1cesWRsfQsg4AIuK+iFglIkbmdTGDtPGe1VLW6oXR3SlZB9lVpIOYSFqPtBdWdsxge+CPke/EW+EvwAfz8Laks4DmUVgHSwFfA86s+Y62roPXS2Jayy8tq7gOgFcqYsraf564iva/Fbinpayy9i/7jGXtf2LF59w+12lmRVll7V/2OYvr4ATycZnC//4DpOS2V37LAcCN7bYRwDvLYiR9GvgYsG9EvFmxvXlQ0jp5mkjr6bGSuDsjYrVC+78c6bkx1eoyRX9/kfqzHyL9SvhqRcwlpP7x10lfyIMr4rYm9eH3nDI3GdixJea9pDM/7iV9aY9rU78PUdFNRDoL6h7mng5WWv8cuwnptLZ7Sf8UK5bELEe6yd872tTpv/OXcwrpLIO3lcT0nE54D7BdXVuSTmu7kfQP9WvSaY5lcbvn4VdJv9qeKImZRjoO1NP+Z1aUdUWu/72k0y6HtVvPpF9HV5SU9RPSqYP3kjaqq1cscxDw07zcu/Jnnm95wAXAYW3abGtS18M9pK6U60pijiR9tx8Cvk36BVv6HW1ZBxMrYlrb//aKuOI6eLgiprX9dyuLa2n/JyrKam3/MRVxre1/RNUye9ZBTXu1tv/BFXHFdXAuJf/7pP/jO3K7/Rx4X0XcEbn955D2mJ4piZlD2pb11OG01rJI3Wm/z202hXQ66lZly2xp/7bdRL4dhZmZLdLdRGZm1kucDMzMzMnAzMycDMzMDCcDMzPDycCWcJLekDRZ0j2S7pL0L23ih0g6vINyb5Y0agHqcUm+ZuYLkvbt9H1mvcXJwJZ0r0TEJhGxMekq9m+1iR8CtE0GC2FkRPyJdDHULe2CzXqbk4HZXG8n3dcHSctLujHvLdyXb7oG6QKwf857Eyfn2K/kmHskfbtQ3t6S7pD0kKQPlC1Q0kWS7gfWzzca+yhwbb4a1axrBvZ1Bcz62OC8EV6GdPXxtnn634HdI+IFSSsDt0kaR7p3/EYRsQmApB1IV81uGREvS1qpUPbAiNhC0o6ke8Vs37rwiPiEpL1J9/i5HPjfiNi7mY9qVs3JwJZ0rxQ27FsBP5a0Een2D9+UtA3pJl/DKL899/bA+ZHvpRQRxfsVXZn/3km6CVuVzUi3k3gv6RYJZl3nZGCWRcSteS9gKOneNEOB90XE65Kmk/YeFsSr+e8blPyv5T2Gb5KeWbFzXt7fJG0XER9euE9htnB8zMAsk7Q+6R7xz5KegPV0TgQfBtbKYS+SHo3Y4wbgIKUHAdHSTVQrIsaTbmw2JSLeQ7ph4aZOBNYXvGdgS7qeYwaQuoYOiIg3JF0EXC3pPtIdY/8IEBHPSvq9pCnAryLiaEmbAJMkvQaMB/5zAZa/KXBPvg370jHvIyXNusZ3LTUzM3cTmZmZk4GZmeFkYGZmOBmYmRlOBmZmhpOBmZnhZGBmZsD/B4khpyMMISYFAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}